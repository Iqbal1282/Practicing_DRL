{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa996fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: 1: nvidia-smi: not found\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "279ecfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "from collections import namedtuple, deque\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "from itertools import cycle, count\n",
    "from textwrap import wrap\n",
    "\n",
    "import matplotlib\n",
    "import subprocess\n",
    "import os.path\n",
    "import tempfile\n",
    "import random\n",
    "import base64\n",
    "import pprint\n",
    "import glob\n",
    "import time\n",
    "import json\n",
    "import sys\n",
    "import gym\n",
    "import io\n",
    "import os\n",
    "import gc\n",
    "\n",
    "from gym import wrappers\n",
    "from subprocess import check_output\n",
    "from IPython.display import HTML\n",
    "\n",
    "LEAVE_PRINT_EVERY_N_SECS = 120\n",
    "ERASE_LINE = '\\x1b[2K'\n",
    "EPS = 1e-6\n",
    "BEEP = lambda: os.system(\"printf '\\a'\")\n",
    "RESULTS_DIR = os.path.join('..', 'results')\n",
    "SEEDS = (12, 34, 56, 78, 90)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29532d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "params = {\n",
    "    'figure.figsize': (15, 8),\n",
    "    'font.size': 24,\n",
    "    'legend.fontsize': 20,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 24,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "}\n",
    "pylab.rcParams.update(params)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43aa3b6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dac2ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_make_env_fn(**kargs):\n",
    "    def make_env_fn(env_name, seed=None, render=None, record=False,\n",
    "                    unwrapped=False, monitor_mode=None, \n",
    "                    inner_wrappers=None, outer_wrappers=None):\n",
    "        mdir = tempfile.mkdtemp()\n",
    "        env = None\n",
    "        if render:\n",
    "            try:\n",
    "                env = gym.make(env_name, render=render)\n",
    "            except:\n",
    "                pass\n",
    "        if env is None:\n",
    "            env = gym.make(env_name)\n",
    "        if seed is not None: env.seed(seed)\n",
    "        env = env.unwrapped if unwrapped else env\n",
    "        if inner_wrappers:\n",
    "            for wrapper in inner_wrappers:\n",
    "                env = wrapper(env)\n",
    "        env = wrappers.Monitor(\n",
    "            env, mdir, force=True, \n",
    "            mode=monitor_mode, \n",
    "            video_callable=lambda e_idx: record) if monitor_mode else env\n",
    "        if outer_wrappers:\n",
    "            for wrapper in outer_wrappers:\n",
    "                env = wrapper(env)\n",
    "        return env\n",
    "    return make_env_fn, kargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98d47a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_videos_html(env_videos, title, max_n_videos=5):\n",
    "    videos = np.array(env_videos)\n",
    "    if len(videos) == 0:\n",
    "        return\n",
    "    \n",
    "    n_videos = max(1, min(max_n_videos, len(videos)))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
    "    videos = videos[idxs,...]\n",
    "\n",
    "    strm = '<h2>{}<h2>'.format(title)\n",
    "    for video_path, meta_path in videos:\n",
    "        video = io.open(video_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "\n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h3>{0}<h3/>\n",
    "        <video width=\"960\" height=\"540\" controls>\n",
    "            <source src=\"data:video/mp4;base64,{1}\" type=\"video/mp4\" />\n",
    "        </video>\"\"\"\n",
    "        strm += html_tag.format('Episode ' + str(meta['episode_id']), encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d56cf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gif_html(env_videos, title, subtitle_eps=None, max_n_videos=4):\n",
    "    videos = np.array(env_videos)\n",
    "    if len(videos) == 0:\n",
    "        return\n",
    "    \n",
    "    n_videos = max(1, min(max_n_videos, len(videos)))\n",
    "    idxs = np.linspace(0, len(videos) - 1, n_videos).astype(int) if n_videos > 1 else [-1,]\n",
    "    videos = videos[idxs,...]\n",
    "\n",
    "    strm = '<h2>{}<h2>'.format(title)\n",
    "    for video_path, meta_path in videos:\n",
    "        basename = os.path.splitext(video_path)[0]\n",
    "        gif_path = basename + '.gif'\n",
    "        if not os.path.exists(gif_path):\n",
    "            ps = subprocess.Popen(\n",
    "                ('ffmpeg', \n",
    "                 '-i', video_path, \n",
    "                 '-r', '7',\n",
    "                 '-f', 'image2pipe', \n",
    "                 '-vcodec', 'ppm',\n",
    "                 '-crf', '20',\n",
    "                 '-vf', 'scale=512:-1',\n",
    "                 '-'), \n",
    "                stdout=subprocess.PIPE)\n",
    "            output = subprocess.check_output(\n",
    "                ('convert',\n",
    "                 '-coalesce',\n",
    "                 '-delay', '7',\n",
    "                 '-loop', '0',\n",
    "                 '-fuzz', '2%',\n",
    "                 '+dither',\n",
    "                 '-deconstruct',\n",
    "                 '-layers', 'Optimize',\n",
    "                 '-', gif_path), \n",
    "                stdin=ps.stdout)\n",
    "            ps.wait()\n",
    "\n",
    "        gif = io.open(gif_path, 'r+b').read()\n",
    "        encoded = base64.b64encode(gif)\n",
    "            \n",
    "        with open(meta_path) as data_file:    \n",
    "            meta = json.load(data_file)\n",
    "\n",
    "        html_tag = \"\"\"\n",
    "        <h3>{0}<h3/>\n",
    "        <img src=\"data:image/gif;base64,{1}\" />\"\"\"\n",
    "        prefix = 'Trial ' if subtitle_eps is None else 'Episode '\n",
    "        sufix = str(meta['episode_id'] if subtitle_eps is None \\\n",
    "                    else subtitle_eps[meta['episode_id']])\n",
    "        strm += html_tag.format(prefix + sufix, encoded.decode('ascii'))\n",
    "    return strm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697b3fd5",
   "metadata": {},
   "source": [
    "# Dueling DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2741cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCQ(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims = (32, 32), \n",
    "                 activation_fc = F.relu):\n",
    "        pass \n",
    "\n",
    "    def _format(self, state):\n",
    "        x = state\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, \n",
    "                             device=self.device, \n",
    "                             dtype=torch.float32)\n",
    "            x = x.unsqueeze(0)\n",
    "        return x\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "\n",
    "        for hidden_layer in self.hidden_layers: \n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x \n",
    "\n",
    "    def numpy_float_to_device(self, variable):\n",
    "        variable = torch.from_numpy(variable).float().to(self.device)\n",
    "        return variable\n",
    "\n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences \n",
    "\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84b2eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyStrategy():\n",
    "    def __init__(self):\n",
    "        self.exploratory_action_taken = False \n",
    "    def select_action(self, model, state):\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().sqeeze()\n",
    "            return np.argmax(q_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b393ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyStrategy():\n",
    "    def __init__(self, epsilon = 0.1):\n",
    "        self.epsilon = epsilon \n",
    "\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False \n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "        if np.random.rand() > self.epsilon: \n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "626bbbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyLinearStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, max_steps=20000):\n",
    "        self.t = 0\n",
    "        self.epsilon = init_epsilon\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.max_steps = max_steps\n",
    "        self.exploratory_action_taken = None\n",
    "        \n",
    "    def _epsilon_update(self):\n",
    "        epsilon = 1 - self.t / self.max_steps\n",
    "        epsilon = (self.init_epsilon - self.min_epsilon) * epsilon + self.min_epsilon\n",
    "        epsilon = np.clip(epsilon, self.min_epsilon, self.init_epsilon)\n",
    "        self.t += 1\n",
    "        return epsilon\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().ipynb_checkpoints/squeeze()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.epsilon = self._epsilon_update()\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "17b26985",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EGreedyLinearStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, max_steps=20000):\n",
    "        self.t = 0\n",
    "        self.epsilon = init_epsilon\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.max_steps = max_steps\n",
    "        self.exploratory_action_taken = None\n",
    "        \n",
    "    def _epsilon_update(self):\n",
    "        epsilon = 1 - self.t / self.max_steps\n",
    "        epsilon = (self.init_epsilon - self.min_epsilon) * epsilon + self.min_epsilon\n",
    "        epsilon = np.clip(epsilon, self.min_epsilon, self.init_epsilon)\n",
    "        self.t += 1\n",
    "        return epsilon\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().ipynb_checkpoints/squeeze()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else: \n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self.epsilon = self._epsilon_update()\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action\n",
    "class EGreedyExpStrategy():\n",
    "    def __init__(self, init_epsilon=1.0, min_epsilon=0.1, decay_steps=20000):\n",
    "        self.epsilon = init_epsilon\n",
    "        self.init_epsilon = init_epsilon\n",
    "        self.decay_steps = decay_steps\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.epsilons = 0.01 / np.logspace(-2, 0, decay_steps, endpoint=False) - 0.01\n",
    "        self.epsilons = self.epsilons * (init_epsilon - min_epsilon) + min_epsilon\n",
    "        self.t = 0\n",
    "        self.exploratory_action_taken = None\n",
    "\n",
    "    def _epsilon_update(self):\n",
    "        self.epsilon = self.min_epsilon if self.t >= self.decay_steps else self.epsilons[self.t]\n",
    "        self.t += 1\n",
    "        return self.epsilon\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).detach().cpu().data.numpy().squeeze()\n",
    "\n",
    "        if np.random.rand() > self.epsilon:\n",
    "            action = np.argmax(q_values)\n",
    "        else:\n",
    "            action = np.random.randint(len(q_values))\n",
    "\n",
    "        self._epsilon_update()\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0bd346b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftMaxStrategy():\n",
    "    def __init__(self, \n",
    "                 init_temp=1.0, \n",
    "                 min_temp=0.3, \n",
    "                 exploration_ratio=0.8, \n",
    "                 max_steps=25000):\n",
    "        self.t = 0\n",
    "        self.init_temp = init_temp\n",
    "        self.exploration_ratio = exploration_ratio\n",
    "        self.min_temp = min_temp\n",
    "        self.max_steps = max_steps\n",
    "        self.exploratory_action_taken = None\n",
    "        \n",
    "    def _update_temp(self):\n",
    "        temp = 1 - self.t / (self.max_steps * self.exploration_ratio)\n",
    "        temp = (self.init_temp - self.min_temp) * temp + self.min_temp\n",
    "        temp = np.clip(temp, self.min_temp, self.init_temp)\n",
    "        self.t += 1\n",
    "        return temp\n",
    "\n",
    "    def select_action(self, model, state):\n",
    "        self.exploratory_action_taken = False\n",
    "        temp = self._update_temp()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q_values = model(state).cpu().detach().data.numpy().squeeze()\n",
    "            scaled_qs = q_values/temp\n",
    "            norm_qs = scaled_qs - scaled_qs.max()            \n",
    "            e = np.exp(norm_qs)\n",
    "            probs = e / np.sum(e)\n",
    "            assert np.isclose(probs.sum(), 1.0)\n",
    "\n",
    "        action = np.random.choice(np.arange(len(probs)), size=1, p=probs)[0]\n",
    "        self.exploratory_action_taken = action != np.argmax(q_values)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d26d12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer():\n",
    "    def __init__(self, \n",
    "                 max_size = 10_000, \n",
    "                 batch_size = 64):\n",
    "        self.ss_mem = np.empty(shape = (max_size), dtype = np.ndarray)\n",
    "        self.as_mem = np.empty(shape = (max_size), dtype = np.ndarray)\n",
    "        self.rs_mem = np.empty(shape = (max_size), dtype = np.ndarray)\n",
    "        self.ps_mem = np.empty(shape = (max_size), dtype = np.ndarray)\n",
    "        self.ds_mem = np.empty(shape = (max_size), dtype = np.ndarray)\n",
    "\n",
    "        self.max_size = max_size \n",
    "        self.batch_size = batch_size \n",
    "        self._idx = 0 \n",
    "        self.size = 0 \n",
    "\n",
    "    def store(self, sample):\n",
    "        s, a, r, p, d = sample \n",
    "        self.ss_mem[self._idx] = s \n",
    "        self.as_mem[self._idx] = a \n",
    "        self.rs_mem[self._idx] = r \n",
    "        self.ps_mem[self._idx] = p \n",
    "        self.ds_mem[self._idx] = d \n",
    "\n",
    "        self._idx += 1 \n",
    "        self._idx = self._idx % self.max_size \n",
    "\n",
    "        self.size += 1 \n",
    "        self.size = min(self.size, self.max_size)\n",
    "        \n",
    "    def sample(self, batch_size = None):\n",
    "        if batch_size == None: \n",
    "            batch_size = self.batch_size \n",
    "        idxs = np.random.choice(\n",
    "            self.size, \n",
    "            batch_size, replace= False\n",
    "        )\n",
    "\n",
    "        experiences = (\n",
    "            np.vstack(self.ss_mem[idxs]),\n",
    "            np.vstack(self.as_mem[idxs]),\n",
    "            np.vstack(self.rs_mem[idxs]),\n",
    "            np.vstack(self.ps_mem[idxs]),\n",
    "            np.vstack(self.ds_mem[idxs])\n",
    "        )\n",
    "        return experiences\n",
    "                                  \n",
    "    def __len__(self):\n",
    "        return self.size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "157e16f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCDuelingQ(nn.Module): \n",
    "    def __init__(self, \n",
    "                 input_dim, \n",
    "                 output_dim, \n",
    "                 hidden_dims = (32, 32), \n",
    "                 activation_fc = F.relu):\n",
    "        super(FCDuelingQ, self).__init__()\n",
    "        self.activation_fc = activation_fc \n",
    "\n",
    "        self.input_layer = nn.Linear(input_dim, hidden_dims[0])\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(hidden_dims) -1): \n",
    "            hidden_layer = nn.Linear(hidden_dims[i], hidden_dims[i+1])\n",
    "            self.hidden_layers.append(hidden_layer)\n",
    "        self.output_value = nn.Linear(hidden_dims[-1], 1)\n",
    "        self.output_layer = nn.Linear(hidden_dims[-1], output_dim)\n",
    "\n",
    "        device = \"cpu\"\n",
    "        if torch.cuda.is_available(): \n",
    "            device = \"cuda:0\"\n",
    "        self.device = torch.device(device)\n",
    "        self.to(self.device)\n",
    "\n",
    "    def _format(self, state): \n",
    "        x = state \n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            x = torch.tensor(x, dtype = torch.float32, device = self.device)\n",
    "        x = x.unsqueeze(0)\n",
    "    def forward(self, state): \n",
    "        x = self._format(state)\n",
    "        x = self.activation_fc(self.input_layer(x))\n",
    "\n",
    "        for hidden_layer in self.hidden_layers: \n",
    "            x = self.activation_fc(hidden_layer(x))\n",
    "        \n",
    "        a = self.output_layer(x)\n",
    "        v = self.output_value(x).expand_as(a)\n",
    "        q = v+a - a.mean(1, keepdim = True).expand_as(a)\n",
    "        return q \n",
    "    \n",
    "    def numpy_float_to_device(self, variable):\n",
    "        variable = torch.from_numpy(variable).float().to(self.device)\n",
    "        return variable\n",
    "\n",
    "    def load(self, experiences):\n",
    "        states, actions, new_states, rewards, is_terminals = experiences\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "        actions = torch.from_numpy(actions).long().to(self.device)\n",
    "        new_states = torch.from_numpy(new_states).float().to(self.device)\n",
    "        rewards = torch.from_numpy(rewards).float().to(self.device)\n",
    "        is_terminals = torch.from_numpy(is_terminals).float().to(self.device)\n",
    "        return states, actions, new_states, rewards, is_terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60622045",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DuelingDDQN(): \n",
    "    def __init__(self, \n",
    "                 replay_buffer_fn, \n",
    "                 value_model_fn, \n",
    "                 value_optimizer_fn, \n",
    "                 value_optimizer_lr, \n",
    "                 max_gradient_norm, \n",
    "                 training_strategy_fn, \n",
    "                 evaluation_strategy_fn, \n",
    "                 n_warmup_batches, \n",
    "                 update_target_every_steps, \n",
    "                 tau):\n",
    "        self.replay_buffer_fn = replay_buffer_fn \n",
    "        self.value_model_fn = value_model_fn \n",
    "        self.value_optimizer_fn = value_optimizer_fn \n",
    "        self.value_optimizer_lr = value_optimizer_lr \n",
    "        self.max_gradient_norm = max_gradient_norm \n",
    "        self.training_strategy_fn = training_strategy_fn \n",
    "        self.evaluation_strategy_fn = self.evaluation_strategy_fn \n",
    "        self.n_warup_batches = n_warmup_batches \n",
    "        self.update_target_every_steps = update_target_every_steps \n",
    "        self.tau = tau \n",
    "\n",
    "    def optimize_model(self, experiences): \n",
    "        states, actions, rewards, next_states, is_terminals = experiences \n",
    "        batch_size = len(is_terminals)\n",
    "\n",
    "        argmax_a_q_sp = self.online_model(next_states).max(1)[1]\n",
    "        q_sp = self.target_model(next_states).detach()\n",
    "\n",
    "        max_a_q_sp = q_sp[np.arange(batch_size), argmax_a_q_sp].unsqueeze(1)\n",
    "        target_q_sa = rewards  + (self.gamma* max_a_q_sp * (1- is_terminals))\n",
    "\n",
    "        q_sa = self.online_model(states).gather(1, actions)\n",
    "\n",
    "        td_error = q_sa - target_q_sa \n",
    "\n",
    "        value_loss = td_error.pow(2).mul(0.5).mean()\n",
    "        self.value_optimizer.zero_grad()\n",
    "        value_loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm(self.online_model.parameters(), self.max_gradient_norm)\n",
    "        self.value_optimizer.step()\n",
    "\n",
    "    def interaction_step(self, state, env):\n",
    "        action = self.training_strategy.select_action(self.online_model, state)\n",
    "        next_state, reward, is_terminal, info = env.step(action)\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2893b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "dueling_ddqn_results = []\n",
    "\n",
    "dueling_ddqn_agents, best_dueling_ddqn_agent_key, best_eval_score = {}, None, float('-inf')\n",
    "\n",
    "for seed in SEEDS: \n",
    "    environment_settings = {\n",
    "        'env_name': 'CartPole-v1',\n",
    "        'gamma': 1.00,\n",
    "        'max_minutes': 20,\n",
    "        'max_episodes': 10000,\n",
    "        'goal_mean_100_reward': 475\n",
    "    }\n",
    "    \n",
    "\n",
    "value_model_fn = lambda nS, nA: FCDuelingQ(nS, nA, hidden_dims=(512, 128))\n",
    "value_optimizer_fn = lambda net, lr: optim.RMSprop(net.parameters(), lr = lr)\n",
    "value_optimizer_lr = 0.0005 \n",
    "max_gradient_norm = float('inf')\n",
    "\n",
    "training_strategy_fn =  lambda: EGreedyExpStrategy(init_epsilon= 1.0, min_epsilon= 0.3, decay_steps= 20_000)\n",
    "evaluation_strategy_fn = lambda: GreedyStrategy()\n",
    "\n",
    "replay_buffer_fn = lambda: ReplayBuffer(max_size = 50_000, batch_size = 64) \n",
    "n_warmup_batches = 5 \n",
    "update_target_every_steps = 1 \n",
    "tau = 0.1 \n",
    "\n",
    "env_name, gamma, max_minutes, \\\n",
    "    max_episodes, goal_mean_100_reward = environment_settings.values()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
