{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "8a0e5076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings ; warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "import gym, gym_walk, gym_aima\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from itertools import cycle, count\n",
    "\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pylab as pylab\n",
    "SEEDS = (12, 34, 56, 78, 90)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "7fd74760",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('fivethirtyeight')\n",
    "params = {\n",
    "    'figure.figsize': (15, 8),\n",
    "    'font.size': 24,\n",
    "    'legend.fontsize': 20,\n",
    "    'axes.titlesize': 28,\n",
    "    'axes.labelsize': 24,\n",
    "    'xtick.labelsize': 20,\n",
    "    'ytick.labelsize': 20\n",
    "}\n",
    "pylab.rcParams.update(params)\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "236a696e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(P, gamma = 1.0, theta = 1e-10):\n",
    "    \"\"\"\n",
    "    Value Iteration algorithm for Markov Decision Processes (MDPs).\n",
    "    \n",
    "    Parameters:\n",
    "    P : dict\n",
    "        Transition probabilities and rewards.\n",
    "    gamma : float\n",
    "        Discount factor.\n",
    "    theta : float\n",
    "        Threshold for convergence.\n",
    "    \n",
    "    Returns:\n",
    "    V : dict\n",
    "        Optimal value function.\n",
    "    policy : dict\n",
    "        Optimal policy.\n",
    "    \"\"\"\n",
    "    V = np.zeros(len(P), dtype=np.float64)\n",
    "\n",
    "    while True: \n",
    "        Q = np.zeros((len(P), len(P[0])), dtype=np.float64)\n",
    "\n",
    "        for s in range(len(P)):\n",
    "            for a in range(len(P[s])):\n",
    "                Q[s, a] = sum(p * (r + gamma * V[s_]) for p, s_, r in P[s][a])\n",
    "\n",
    "        if np.max(np.abs(V - np.max(Q, axis = 1))) < theta:\n",
    "            break \n",
    "    pi = lambda s: np.argmax(Q[s])\n",
    "    return V, pi, Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "78312a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_policy(pi, P, action_symbols=('<', 'v', '>', '^'), n_cols=4, title='Policy:'):\n",
    "    print(title)\n",
    "    arrs = {k:v for k,v in enumerate(action_symbols)}\n",
    "    for s in range(len(P)):\n",
    "        a = pi(s)\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), arrs[a].rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "4c06e7f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_state_value_function(V, P, n_cols=4, prec=3, title='State-value function:'):\n",
    "    print(title)\n",
    "    for s in range(len(P)):\n",
    "        v = V[s]\n",
    "        print(\"| \", end=\"\")\n",
    "        if np.all([done for action in P[s].values() for _, _, _, done in action]):\n",
    "            print(\"\".rjust(9), end=\" \")\n",
    "        else:\n",
    "            print(str(s).zfill(2), '{}'.format(np.round(v, prec)).rjust(6), end=\" \")\n",
    "        if (s + 1) % n_cols == 0: print(\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "3a6bcf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_action_value_function(Q, \n",
    "                                optimal_Q=None, \n",
    "                                action_symbols=('<', '>'), \n",
    "                                prec=3, \n",
    "                                title='Action-value function:'):\n",
    "    vf_types=('',) if optimal_Q is None else ('', '*', 'err')\n",
    "    headers = ['s',] + [' '.join(i) for i in list(itertools.product(vf_types, action_symbols))]\n",
    "    print(title)\n",
    "    states = np.arange(len(Q))[..., np.newaxis]\n",
    "    arr = np.hstack((states, np.round(Q, prec)))\n",
    "    if not (optimal_Q is None):\n",
    "        arr = np.hstack((arr, np.round(optimal_Q, prec), np.round(optimal_Q-Q, prec)))\n",
    "    print(tabulate(arr, headers, tablefmt=\"fancy_grid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd41fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics_from_tracks(env, gamma, goal_state, optimal_Q, pi_track, coverage = 0.1):\n",
    "    \"\"\"\n",
    "    Calculate metrics from the tracks of the agent's performance.\n",
    "\n",
    "    Parameters:\n",
    "    env : gym.Env\n",
    "        The environment.\n",
    "    gamma : float\n",
    "        Discount factor.\n",
    "    goal_state : int\n",
    "        The goal state index.\n",
    "    optimal_Q : np.ndarray\n",
    "        Optimal action-value function.\n",
    "    pi_track : list\n",
    "        List of policies tracked during the episodes.\n",
    "    coverage : float\n",
    "        Coverage threshold for the policy.\n",
    "\n",
    "    Returns:\n",
    "    metrics : dict\n",
    "        Dictionary containing various performance metrics.\n",
    "    \"\"\"\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e5ca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
